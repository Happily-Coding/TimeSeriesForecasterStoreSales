{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea of this competition, is to predict sales for mnay product families for stores in ecuador.\n",
    "#Train and test share: date, store_nbr, family(product_family), onpromotion (number of products of the family that were on promotion)\n",
    "#Train also has the sales.\n",
    "#Test is what we are trying to predict in the competition\n",
    "\n",
    "#Stores contains store metadata, we might want to join that data into our train and test tables to improve predictions if they are not redundant with the store_nbr (aka if at least some of the stores share their values).\n",
    " #stores metadata\n",
    "#type and cluster are not redundant. see stores_df[['type', 'cluster']].drop_duplicates()#.pivot(columns='cluster')\n",
    "\n",
    "#Daily oil price, keep in mind oil price affects ecuador economical health.\n",
    "#We probably want to join it into our dataframe, but keep in mind it seems to have some null values. We probably want to fill downwards so that it takes its value from the previous date but well have to see. We could also average between the next and previous day.\n",
    "\n",
    "\n",
    "#While its not clear it seems to be the total transactions of the store on that day\n",
    "\n",
    "\n",
    "#A table containing holiday events. \n",
    "#its worth noting that some events were transfered in date so we need to take into account hte actual date. transfered column True means that the holiday wasnt actually celebrated that date.\n",
    "#Also, keep in mind that some seem to be regional and some local. This could mean that we need to take into account the region in considering them.\n",
    "#Some special_days were of type bridge, meaning  they are extra days added to the same holiday.\n",
    "#This are often compensated by making some not working days(ie saturday) working days. This are of type workday.\n",
    "#holiday_type additional means that it isnt an actual holiday, but actually the extension of one.\n",
    "\n",
    "#Additonally to these tables wages on the public sector are paid on the 15th and last day of each month which could affect supermarket sales\n",
    "#A great earthquiake struck ecuador on april 16 2016 which resulted in donations possibly affecting sales.\n",
    "\n",
    "\n",
    "#TLDR we need to add many features to the train and test df.\n",
    "#First lets make some column names easier to understand. Specially after joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_preparation as data_prep\n",
    "#import tensorflow as tf\n",
    "#import keras\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Reload all modules imported with %aimport every time before executing the Python code typed. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload explicit\n",
    "%aimport data_preparation \n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "#TODO move the eda process to another file? Might need to move the processing functions to an auxiliary file aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, elements_to_predict_df, stores_df, oil_df, transactions_df, special_days_df, sample_submission_df = data_prep.get_raw_dfs()#data_prep.rename_raw_dfs_cols(*data_prep.get_raw_dfs())\n",
    "train_y_truth:pd.Series = train_df.pop('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_preparation import rolling_window_dataset\n",
    "#rolling_window_dataset(train_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .... (step 1 of 8) Processing rename_columns, total=   0.1s\n",
      "[Pipeline] .. (step 2 of 8) Processing merge_dataframes, total=   0.4s\n",
      "[Pipeline]  (step 3 of 8) Processing fill_missing_oil_values, total=   0.0s\n",
      "[Pipeline]  (step 4 of 8) Processing refine_special_day_reason, total=   0.1s\n",
      "[Pipeline]  (step 5 of 8) Processing replace_date_with_date_related_columns, total=   0.1s\n",
      "[Pipeline] .. (step 6 of 8) Processing reorder_features, total=   0.0s\n",
      "[Pipeline] .. (step 7 of 8) Processing prepare_features, total=   5.9s\n",
      "[Pipeline] .... (step 8 of 8) Processing window_dataset, total=   1.2s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "pipeline = data_prep.create_pipeline(stores_df, oil_df, transactions_df, special_days_df, window_size=2, verbose=True) #El de date tardo mucho.\n",
    "processed_train_df = pipeline.fit_transform(train_df[:100000], train_y_truth[:100000])\n",
    "#scores = cross_val_score(pipeline, train_df, train_y_truth, cv=5)\n",
    "#processed_elements_to_predict = pipeline.transform(elements_to_predict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99999 entries, 1 to 99999\n",
      "Columns: 324 entries, oil_price to products_of_family_on_promotion\n",
      "dtypes: bool(302), float64(4), int32(10), int64(8)\n",
      "memory usage: 41.8 MB\n"
     ]
    }
   ],
   "source": [
    "# We need to monitor memory usage because it could be more than we can handle.\n",
    "processed_train_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters\n",
    "    hyperparams = { #When using pipelines, you need to prefix the parameters depending on which part of the pipeline they refer to with the name of the respective component\n",
    "        'regressor__fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'regressor__normalize': trial.suggest_categorical('normalize', [True, False]),\n",
    "    }\n",
    "\n",
    "    # Set the hyperparameters to the pipeline\n",
    "    fold_pipeline = data_prep.create_pipeline().set_params(**hyperparams)\n",
    "\n",
    "    # Perform cross-validation and return the mean score\n",
    "    scores = cross_val_score(fold_pipeline, features_df, target_df, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    #Store additional results for the trial\n",
    "    trial.set_user_attr('cv_scores', scores)\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "sampler = optuna.samplers.TPESampler() #The TPESampler is a class in Optuna that implements the Tree-structured Parzen Estimator (TPE) algorithm, which is a kind of genetic algorithm.\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "for i, trial in enumerate(study.trials):\n",
    "    print(f'Trial {i}:')\n",
    "    print(f'  Params: {trial.params}')\n",
    "    print(f'  Scores: {trial.user_attrs[\"cv_scores\"]}')\n",
    "\n",
    "best_params = study.best_params\n",
    "print(best_params)\n",
    "\n",
    "pipeline = data_prep.create_pipeline().set_params(best_params)\n",
    "#it could be a good idea here to do cv (maybe with cv_predict) and analyze the predictions during cv in order to find where it fails, and compare it to other models to make enssambles.\n",
    "#Maybe what we could compare is the difference in the prediction for each value, and sum the difference in those diferences between the models.\n",
    "#The number of models in the ensamble could be tuned in another trial.\n",
    "#The weights in voting in the ensamble could be tuned in another trial.\n",
    "\n",
    "pipeline.set_params(**best_params)\n",
    "pipeline.fit(train_df, dataset_y)\n",
    "\n",
    "predictions = pipeline.predict(elements_to_predict_df)\n",
    "\n",
    "\n",
    "# Analyze the performance\n",
    "# This part depends on what kind of analysis you want to do.\n",
    "# For example, you can calculate the prediction error for each sample:\n",
    "errors = target_df - pipeline.predict(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
