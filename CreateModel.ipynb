{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea of this competition, is to predict sales for mnay product families for stores in ecuador.\n",
    "#Train and test share: date, store_nbr, family(product_family), onpromotion (number of products of the family that were on promotion)\n",
    "#Train also has the sales.\n",
    "#Test is what we are trying to predict in the competition\n",
    "\n",
    "#Stores contains store metadata, we might want to join that data into our train and test tables to improve predictions if they are not redundant with the store_nbr (aka if at least some of the stores share their values).\n",
    " #stores metadata\n",
    "#type and cluster are not redundant. see stores_df[['type', 'cluster']].drop_duplicates()#.pivot(columns='cluster')\n",
    "\n",
    "#Daily oil price, keep in mind oil price affects ecuador economical health.\n",
    "#We probably want to join it into our dataframe, but keep in mind it seems to have some null values. We probably want to fill downwards so that it takes its value from the previous date but well have to see. We could also average between the next and previous day.\n",
    "\n",
    "\n",
    "#While its not clear it seems to be the total transactions of the store on that day\n",
    "\n",
    "\n",
    "#A table containing holiday events. \n",
    "#its worth noting that some events were transfered in date so we need to take into account hte actual date. transfered column True means that the holiday wasnt actually celebrated that date.\n",
    "#Also, keep in mind that some seem to be regional and some local. This could mean that we need to take into account the region in considering them.\n",
    "#Some special_days were of type bridge, meaning  they are extra days added to the same holiday.\n",
    "#This are often compensated by making some not working days(ie saturday) working days. This are of type workday.\n",
    "#holiday_type additional means that it isnt an actual holiday, but actually the extension of one.\n",
    "\n",
    "#Additonally to these tables wages on the public sector are paid on the 15th and last day of each month which could affect supermarket sales\n",
    "#A great earthquiake struck ecuador on april 16 2016 which resulted in donations possibly affecting sales.\n",
    "\n",
    "\n",
    "#TLDR we need to add many features to the train and test df.\n",
    "#First lets make some column names easier to understand. Specially after joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "with open('./.kaggle/kaggle.json') as credentials_file:\n",
    "    credentials_dict = json.load(credentials_file)\n",
    "    os.environ['KAGGLE_USERNAME'] = credentials_dict['username']\n",
    "    os.environ['KAGGLE_KEY'] = credentials_dict['key']\n",
    "    import kaggle\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "#TODO move the eda process to another file? Might need to move the processing functions to an auxiliary file aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    kaggle.api.competition_download_files(competition='store-sales-time-series-forecasting', path='./dataset', force=False, quiet=True)\n",
    "    with ZipFile('./dataset/store-sales-time-series-forecasting.zip') as dataset_zip:\n",
    "        dataset_zip.extractall('./dataset')\n",
    "\n",
    "def get_raw_dfs() ->tuple:\n",
    "    \"\"\"Get dataframes containing all the info in the dataset.\"\"\"\n",
    "    train_df_inc = pd.read_csv('./dataset/train.csv', index_col='id')\n",
    "    test_df_inc = pd.read_csv('./dataset/test.csv', index_col='id')\n",
    "    stores_df = pd.read_csv('./dataset/stores.csv', index_col='store_nbr')\n",
    "    oil_df = pd.read_csv('./dataset/oil.csv') \n",
    "    transactions_df = pd.read_csv('./dataset/transactions.csv')\n",
    "    holiday_events_df = pd.read_csv('./dataset/holidays_events.csv')\n",
    "    sample_submission_df = pd.read_csv('./dataset/sample_submission.csv')\n",
    "    return train_df_inc, test_df_inc, stores_df, oil_df, transactions_df, holiday_events_df, sample_submission_df\n",
    "\n",
    "def rename_raw_dfs_cols(train_df_inc, test_df_inc, stores_df, oil_df, transactions_df, holiday_events_df, sample_submission_df) ->tuple:\n",
    "    \"\"\"Rename the columns of the raw dataframes so they are more easily understandable\"\"\"\n",
    "    train_df_inc = train_df_inc.rename(columns={'family':'product_family', 'onpromotion':'products_of_family_on_promotion'})\n",
    "    test_df_inc = test_df_inc.rename(columns={'family':'product_family'})\n",
    "    oil_df = oil_df.rename(columns={'dcoilwtico':'oil_price'})\n",
    "    stores_df = stores_df.rename(columns={'type':'store_type', 'cluster':'store_cluster', 'city':'store_city', 'state':'store_state'})\n",
    "    transactions_df = transactions_df.rename(columns={'transactions':'all_products_transactions'})\n",
    "    special_days_df = holiday_events_df.rename(columns={'type':'day_type', 'locale':'special_day_locale_type', 'locale_name':'special_day_locale','description':'special_day_reason', 'transferred':'special_day_transferred'})\n",
    "    return train_df_inc, test_df_inc, stores_df, oil_df, transactions_df, special_days_df, sample_submission_df\n",
    "\n",
    "def complete_features_dataset(features_df:pd.DataFrame, stores_df:pd.DataFrame, oil_df:pd.DataFrame, transactions_df:pd.DataFrame, special_days_df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"Add relevant columns from the auxiliary dataframes into a features dataset, be it the train set or the test set.\"\"\"\n",
    "    full_features_df = features_df.merge(oil_df, on='date',how='left')\n",
    "    full_features_df = full_features_df.merge(stores_df, on=['store_nbr'], how='left') #Need to also consider date. 'date', nvm\n",
    "    full_features_df = full_features_df.merge(special_days_df, on='date', how='left')\n",
    "    full_features_df = full_features_df.merge(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "    return full_features_df\n",
    "\n",
    "def reorder_features_dataset(features_df):\n",
    "    \"\"\"Reorder the columns in the feature dataframe so the table becomes easier to understand and inspect. Does not affect the rows.\"\"\"\n",
    "    return features_df # TODO Order to columns more logically\n",
    "\n",
    "def one_hot_encode_necessary_features(features_df): #TODO change call to after the creation of the proper columns, and the columns to match it.\n",
    "    \"\"\"One hot encodes the columns, using their name as prefix, and adding them into the same place the original was, while removing the original \"\"\"\n",
    "    names_of_columns_to_ohe= [ 'store_nbr', 'product_family', 'store_city','store_state',\n",
    "                      'store_type', 'day_type', 'special_day_locale_type', 'special_day_locale',\n",
    "                       'special_day_reason', 'special_day_transferred']\n",
    "                    #holiday_transferred may be better vectorized. Day type might be too. holiday_locale_type too.\n",
    "                    #This is because their values might have meaning in the order.\n",
    "    \n",
    "    for name_of_column_to_ohe in names_of_columns_to_ohe:\n",
    "        index_of_col_to_ohe = features_df.columns.get_loc(name_of_column_to_ohe)\n",
    "        one_hot_encoded_column:pd.DataFrame = pd.get_dummies(features_df[name_of_column_to_ohe], dummy_na=True, prefix=name_of_column_to_ohe)\n",
    "        \n",
    "        # Split the dataframe into two parts: before and after the position of the original column\n",
    "        columns_before_col_to_ohe = features_df.iloc[:, :index_of_col_to_ohe]\n",
    "        columns_after_col_to_ohe = features_df.iloc[:, index_of_col_to_ohe+1:] #Exclude the current column\n",
    "\n",
    "        # Concatenate the two parts with the one-hot encoded dataframe in between\n",
    "        features_df = pd.concat([columns_before_col_to_ohe, one_hot_encoded_column, columns_after_col_to_ohe], axis=1)\n",
    "\n",
    "    return features_df\n",
    "    #TODO #return the dataset and the references\n",
    "\n",
    "def process_numerical_features(features_df, normalizers, is_test_data): #Should take the normalizer as parameter.\n",
    "    \"\"\"Either normalize or standardize variables depending on their distribution. Also handle missing values where necessary. \"\"\"\n",
    "    print(features_df.columns)\n",
    "    numerical_variable_names = ['oil_price', 'all_products_transactions']\n",
    "\n",
    "    #Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n",
    "    #Its more robust than standarization since it doesnt require the distribution to be gaussian.\n",
    "    #MinMaxScaler is a scikitlearn normalizer.\n",
    "\n",
    "    for variable_name in numerical_variable_names: #TODO MAKE IT CHECK IF ITS TEST AND TRANSFORM INSTEAD OF FIT TRANSFORM\n",
    "        features_df[variable_name + '_standardized'] = normalizers[variable_name].fit_transform(features_df[variable_name])\n",
    "        features_df[variable_name + '_normalized'] = normalizers[variable_name].fit_transform(features_df[variable_name]) #hadd double brakcets\n",
    "\n",
    "\n",
    "    return features_df\n",
    "    #TODO #return the dataset and the normalizer\n",
    "\n",
    "def fill_missing_values(features_df):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "def fill_missing_oil_values(features_df):\n",
    "    # Oil price highly varies over time, making average imputing inaddecuate.\n",
    "    # While we could use the average between the previous day and the next day, that wouldnt work for forecasting.\n",
    "    # Instead just use the last known price, sinceFill down\n",
    "    features_df['oil_price'] = features_df['oil_price'].fillna(method='ffill')\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def refine_special_day_reason(features_df):\n",
    "    \"\"\"Assign the same reason to special days that have the same reason but with a small variation, storing otherwise lost information in other columns\"\"\"\n",
    "\n",
    "    # Create a special_day_reason subtype column. Extract the subtype of mundial de futbol brasil into a subtype column, since they all have a common reason.\n",
    "    features_df['special_day_reason_subtype'] = features_df['special_day_reason'].apply(lambda x: x.replace('Mundial de futbol Brasil: ', '').replace('Mundial de futbol Brasil', '') if (type(x) == str) and (x.startswith('Mundial de futbol Brasil')) else '')\n",
    "    \n",
    "    # Create a function to extract the offset in any reasons that end with -number or +number\n",
    "    # Function to process the special_day_offset column\n",
    "    def get_special_day_offset(value):\n",
    "        if type(value) == str:\n",
    "            match = re.search(r'([-+]\\d+)$', value)\n",
    "            if match:\n",
    "                # Return the number with the sign as an integer\n",
    "                return int(match.group())\n",
    "\n",
    "            # If the condition is not met, return NaN\n",
    "        return 0\n",
    "    \n",
    "    #Use the function to create a special_day_offset_column\n",
    "    features_df['special_day_offset'] = features_df['special_day_reason'].apply(get_special_day_offset)\n",
    "\n",
    "\n",
    "    # Now that the data that is not stored elsewhere has been stored create a function to clean the special_day_reason column\n",
    "    def process_special_day_reason_value(value):\n",
    "        '''A function to process the special_day_reason elements that can be applied element-wise using pandas.'''\n",
    "\n",
    "        if type(value) == str:\n",
    "            #If the reason contains a locale, eliminate the locale part of the reason, since its already stored in locale column.\n",
    "            prefixes = ['Fundacion', 'Independencia', 'Provincializacion', 'Cantonizacion', 'Translado']\n",
    "            for prefix in prefixes:\n",
    "                if value.startswith(prefix):\n",
    "                    value = prefix\n",
    "            \n",
    "            # If if value ends with - followed by any number or + followed by any number, aka if it has an offset, eliminate the offset part of the reason, since its already stored in the offset column.  \n",
    "            match = re.search(r'([-+]\\d+)$', value) # Check if value ends with - followed by any number or + followed by any number\n",
    "            if match:\n",
    "                return value.replace(match.group(), '') # Return the string without the number and sign\n",
    "            \n",
    "            # If the reason is mundial de futbol brasil, eliminate additional detaisl since they are already stored in the reason_subtype column. #Todo verify synthax\n",
    "            if 'Mundial de futbol Brasil' in value: #value.contains('Mundial de futbol Brasil'):\n",
    "                return 'Mundial de futbol Brasil'\n",
    "        \n",
    "        # If the reason does not need any preprocessing just return it\n",
    "        return value\n",
    "\n",
    "    #And now apply that function element-wise to cleanup the special_day_reason column to not store redundant info and unify reasons that are actually the same.\n",
    "    features_df['special_day_reason'] = features_df['special_day_reason'].apply(process_special_day_reason_value)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def replace_date_with_date_related_columns(features_df):\n",
    "    \"\"\"\"Adds many date_related_columns \"\"\"\n",
    "\n",
    "    features_df['date'] = pd.to_datetime(features_df['date'])\n",
    "\n",
    "    #Calculate the absolute date (from unix epoch), which is a standard start date used by many siystems and libraries working with time series data.\n",
    "    #While this would work its probably worth for the nn to learn.\n",
    "    #UNIX_EPOCH = pd.Timestamp(\"1970-01-01\")\n",
    "    #features_df['absolute_day_number'] = (features_df['date'] - UNIX_EPOCH) // pd.Timedelta('1D')\n",
    "\n",
    "    #Calculate the number of days since the first date in the dataset.\n",
    "    features_df['df_start_absolute_day'] = (features_df['date'] - features_df['date'].min()) // pd.Timedelta('1D')\n",
    "\n",
    "    features_df['day_of_year'] = features_df['date'].dt.dayofyear\n",
    "    features_df['day_of_month'] = features_df['date'].dt.day\n",
    "    features_df['day_of_week'] = features_df['date'].dt.dayofweek\n",
    "\n",
    "    features_df['is_15th'] = (features_df['date'].dt.day == 15).astype(int)\n",
    "    features_df['is_last_day_of_month'] = (features_df['date'] == features_df['date'] + MonthEnd(1)).astype(int)\n",
    "\n",
    "    features_df = features_df.drop(columns=['date'])\n",
    "\n",
    "\n",
    "\n",
    "def prepare_features_df(features_df:pd.DataFrame, stores_df, oil_df, transactions_df, special_days_df, normalizers):\n",
    "    \"\"\"Call all the preprocessing methods in order, and prepare a features dataframe for deep learning, be it the train set or the test set\"\"\"\n",
    "    #TODO READ and rename columns of the dataset, and then process them here.\n",
    "    features_df = complete_features_dataset(features_df, stores_df, oil_df, transactions_df, special_days_df) #Careful, do not move this later, since the rest of the methods need access to all the columns.\n",
    "    features_df = reorder_features_dataset(features_df) #Careful, do not move this later, the rest of the methods try to respect the order of the dataframe, so theyll end up in messy places otherwise.\n",
    "    features_df = fill_missing_oil_values(features_df)     \n",
    "    features_df = refine_special_day_reason(features_df)\n",
    "    features_df = process_numerical_features(features_df) #Rename or include fill missing oil values here?\n",
    "    features_df = one_hot_encode_necessary_features(features_df) #Careful moving this earlier since one hot encoding properly requires categorical variables to have been processed.\n",
    "    features_df = replace_date_with_date_related_columns(features_df) #Careful, moving calling this earlier could be problematic since it eliminates date column.\n",
    "\n",
    "    return features_df\n",
    "\n",
    "def window_dataset():\n",
    "    \"\"\"Window the dataset so that it can be used for training a neural network.\"\"\"\n",
    "    #Aka create a window of days with values and then the unknown value for the next day. and then slide that window forward to create another data point.\n",
    "    #TODO\n",
    "    return\n",
    "\n",
    "def remove_seasonality():\n",
    "    #perform fourier transform? \n",
    "    #TODO?\n",
    "    return\n",
    "\n",
    "def train_val_split_dataset():\n",
    "    \"\"\"Split the datataset so that it can be used for training and then validation\"\"\"\n",
    "    #TODO, probably the most sensible approach for a simple time series analysis is to use the last data for validation,\n",
    "    #though i can imagine other approaches which could lead to better peformance, for example masking like bert does for text data.\n",
    "    #Or creating crossval sets by skipping part of the time before prediction\n",
    "    return\n",
    "\n",
    "def prepare_datasets():\n",
    "    train_df_inc, test_df_inc, stores_df, oil_df, transactions_df, special_days_df, sample_submission_df = rename_raw_dfs_cols(*get_raw_dfs())\n",
    "    train_y_truth = train_df_inc.pop('sales')\n",
    "    normalizers = {'oil_price':MinMaxScaler(), 'all_products_transactions':MinMaxScaler()}\n",
    "    train_df, test_df = prepare_features_df(train_df_inc, stores_df, oil_df, transactions_df, special_days_df, normalizers, False), prepare_features_df(test_df_inc, stores_df, oil_df, transactions_df, special_days_df, normalizers, True)\n",
    "    return train_df, test_df, train_y_truth, sample_submission_df\n",
    "\n",
    "\n",
    "train_df, test_df, train_y_truth, sample_dataset = prepare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
