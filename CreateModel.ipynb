{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea of this competition, is to predict sales for mnay product families for stores in ecuador.\n",
    "#Train and test share: date, store_nbr, family(product_family), onpromotion (number of products of the family that were on promotion)\n",
    "#Train also has the sales.\n",
    "#Test is what we are trying to predict in the competition\n",
    "\n",
    "#Stores contains store metadata, we might want to join that data into our train and test tables to improve predictions if they are not redundant with the store_nbr (aka if at least some of the stores share their values).\n",
    " #stores metadata\n",
    "#type and cluster are not redundant. see stores_df[['type', 'cluster']].drop_duplicates()#.pivot(columns='cluster')\n",
    "\n",
    "#Daily oil price, keep in mind oil price affects ecuador economical health.\n",
    "#We probably want to join it into our dataframe, but keep in mind it seems to have some null values. We probably want to fill downwards so that it takes its value from the previous date but well have to see. We could also average between the next and previous day.\n",
    "\n",
    "\n",
    "#While its not clear it seems to be the total transactions of the store on that day\n",
    "\n",
    "\n",
    "#A table containing holiday events. \n",
    "#its worth noting that some events were transfered in date so we need to take into account hte actual date. transfered column True means that the holiday wasnt actually celebrated that date.\n",
    "#Also, keep in mind that some seem to be regional and some local. This could mean that we need to take into account the region in considering them.\n",
    "#Some special_days were of type bridge, meaning  they are extra days added to the same holiday.\n",
    "#This are often compensated by making some not working days(ie saturday) working days. This are of type workday.\n",
    "#holiday_type additional means that it isnt an actual holiday, but actually the extension of one.\n",
    "\n",
    "#Additonally to these tables wages on the public sector are paid on the 15th and last day of each month which could affect supermarket sales\n",
    "#A great earthquiake struck ecuador on april 16 2016 which resulted in donations possibly affecting sales.\n",
    "\n",
    "\n",
    "#TLDR we need to add many features to the train and test df.\n",
    "#First lets make some column names easier to understand. Specially after joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_preparation as data_prep\n",
    "#import tensorflow as tf\n",
    "#import keras\n",
    "#from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload all modules imported with %aimport every time before executing the Python code typed. https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html\n",
    "%load_ext autoreload\n",
    "%autoreload explicit\n",
    "%aimport data_preparation \n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "#TODO move the eda process to another file? Might need to move the processing functions to an auxiliary file aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset for the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that I intentionally merge the parts of the dataset outside the pipeline, so the pipeline will remain usable even if the data is provided in a different format, for example in a unified table. or as a manually created df.\n",
    "TODO consider this again since its not a valid reason, considering that pipeline slicing is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, elements_to_predict_df, stores_df, oil_df, transactions_df, special_days_df, sample_submission_df = data_prep.rename_raw_dfs_cols(*data_prep.get_raw_dfs())\n",
    "train_y_truth:pd.Series = train_df.pop('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\CreateModel.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/uriel.salvio/Documents/Python%20Projects/TimeSeriesForecasterStoreSales/CreateModel.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipeline \u001b[39m=\u001b[39m data_prep\u001b[39m.\u001b[39mcreate_pipeline(stores_df, oil_df, transactions_df, special_days_df, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/uriel.salvio/Documents/Python%20Projects/TimeSeriesForecasterStoreSales/CreateModel.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m processed_train_df \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit_transform(train_df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/uriel.salvio/Documents/Python%20Projects/TimeSeriesForecasterStoreSales/CreateModel.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m processed_elements_to_predict \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mtransform(elements_to_predict_df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/uriel.salvio/Documents/Python%20Projects/TimeSeriesForecasterStoreSales/CreateModel.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m display(pipeline)\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:535\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \n\u001b[0;32m    494\u001b[0m \u001b[39mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[39m    Transformed samples.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    534\u001b[0m routed_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_method_params(method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m, props\u001b[39m=\u001b[39mparams)\n\u001b[1;32m--> 535\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, routed_params)\n\u001b[0;32m    537\u001b[0m last_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\n\u001b[0;32m    538\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:408\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, routed_params)\u001b[0m\n\u001b[0;32m    406\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    407\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    409\u001b[0m     cloned_transformer,\n\u001b[0;32m    410\u001b[0m     X,\n\u001b[0;32m    411\u001b[0m     y,\n\u001b[0;32m    412\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    413\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    414\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[0;32m    415\u001b[0m     params\u001b[39m=\u001b[39;49mrouted_params[name],\n\u001b[0;32m    416\u001b[0m )\n\u001b[0;32m    417\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py:1303\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m   1302\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1303\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mfit_transform\u001b[39;49m\u001b[39m\"\u001b[39;49m, {}))\n\u001b[0;32m   1304\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1305\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m, {}))\u001b[39m.\u001b[39mtransform(\n\u001b[0;32m   1306\u001b[0m             X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m\"\u001b[39m, {})\n\u001b[0;32m   1307\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 273\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    275\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    278\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    279\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\sklearn\\base.py:1061\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1047\u001b[0m             (\n\u001b[0;32m   1048\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis object (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m) has a `transform`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m   1057\u001b[0m         )\n\u001b[0;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1060\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1061\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[0;32m   1062\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1063\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 273\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    275\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    278\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    279\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:242\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \n\u001b[0;32m    230\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39m    Transformed input.\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    241\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_input(X, reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(X, func\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, kw_args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkw_args)\n\u001b[0;32m    244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(out, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names_out \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     \u001b[39m# check the consistency between the column names of the output and the\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39m# one generated by `get_feature_names_out`\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mcolumns) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_feature_names_out()):\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:348\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[1;34m(self, X, func, kw_args)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     func \u001b[39m=\u001b[39m _identity\n\u001b[1;32m--> 348\u001b[0m \u001b[39mreturn\u001b[39;00m func(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m(kw_args \u001b[39mif\u001b[39;49;00m kw_args \u001b[39melse\u001b[39;49;00m {}))\n",
      "File \u001b[1;32mc:\\Users\\uriel.salvio\\Documents\\Python Projects\\TimeSeriesForecasterStoreSales\\data_preparation.py:39\u001b[0m, in \u001b[0;36mmerge_data_sources\u001b[1;34m(features_df, stores_df, oil_df, transactions_df, special_days_df)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_data_sources\u001b[39m(features_df:pd\u001b[39m.\u001b[39mDataFrame, stores_df:pd\u001b[39m.\u001b[39mDataFrame, oil_df:pd\u001b[39m.\u001b[39mDataFrame, transactions_df:pd\u001b[39m.\u001b[39mDataFrame, special_days_df:pd\u001b[39m.\u001b[39mDataFrame)\u001b[39m-\u001b[39m\u001b[39m>\u001b[39mpd\u001b[39m.\u001b[39mDataFrame:\n\u001b[0;32m     38\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Add relevant columns from the auxiliary dataframes into a features dataset, be it the train set or the test set.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     full_features_df \u001b[39m=\u001b[39m features_df\u001b[39m.\u001b[39;49mmerge(oil_df, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m,how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m     full_features_df \u001b[39m=\u001b[39m full_features_df\u001b[39m.\u001b[39mmerge(stores_df, on\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mstore_nbr\u001b[39m\u001b[39m'\u001b[39m], how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     41\u001b[0m     full_features_df \u001b[39m=\u001b[39m full_features_df\u001b[39m.\u001b[39mmerge(special_days_df, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'merge'"
     ]
    }
   ],
   "source": [
    "pipeline = data_prep.create_pipeline(stores_df, oil_df, transactions_df, special_days_df, verbose=False)\n",
    "processed_train_df = pipeline.fit_transform(train_df)\n",
    "processed_elements_to_predict = pipeline.transform(elements_to_predict_df)\n",
    "\n",
    "display(pipeline)\n",
    "display(processed_train_df.columns)\n",
    "display(processed_train_df.head(5))\n",
    "#TODO use the outpout tuple of rename dfs as input without keyword args for the merger of the pipeline. Modify the merge function acrodingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters\n",
    "    hyperparams = { #When using pipelines, you need to prefix the parameters depending on which part of the pipeline they refer to with the name of the respective component\n",
    "        'regressor__fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),\n",
    "        'regressor__normalize': trial.suggest_categorical('normalize', [True, False]),\n",
    "    }\n",
    "\n",
    "    # Set the hyperparameters to the pipeline\n",
    "    fold_pipeline = data_prep.create_pipeline().set_params(**hyperparams)\n",
    "\n",
    "    # Perform cross-validation and return the mean score\n",
    "    scores = cross_val_score(fold_pipeline, features_df, target_df, cv=5, scoring='neg_mean_squared_error')\n",
    "    \n",
    "    #Store additional results for the trial\n",
    "    trial.set_user_attr('cv_scores', scores)\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "sampler = optuna.samplers.TPESampler() #The TPESampler is a class in Optuna that implements the Tree-structured Parzen Estimator (TPE) algorithm, which is a kind of genetic algorithm.\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "for i, trial in enumerate(study.trials):\n",
    "    print(f'Trial {i}:')\n",
    "    print(f'  Params: {trial.params}')\n",
    "    print(f'  Scores: {trial.user_attrs[\"cv_scores\"]}')\n",
    "\n",
    "best_params = study.best_params\n",
    "print(best_params)\n",
    "\n",
    "pipeline = data_prep.create_pipeline().set_params(best_params)\n",
    "#it could be a good idea here to do cv (maybe with cv_predict) and analyze the predictions during cv in order to find where it fails, and compare it to other models to make enssambles.\n",
    "#Maybe what we could compare is the difference in the prediction for each value, and sum the difference in those diferences between the models.\n",
    "#The number of models in the ensamble could be tuned in another trial.\n",
    "#The weights in voting in the ensamble could be tuned in another trial.\n",
    "\n",
    "pipeline.set_params(**best_params)\n",
    "pipeline.fit(train_df, dataset_y)\n",
    "\n",
    "predictions = pipeline.predict(elements_to_predict_df)\n",
    "\n",
    "\n",
    "# Analyze the performance\n",
    "# This part depends on what kind of analysis you want to do.\n",
    "# For example, you can calculate the prediction error for each sample:\n",
    "errors = target_df - pipeline.predict(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
