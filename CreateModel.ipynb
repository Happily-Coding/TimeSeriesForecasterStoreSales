{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea of this competition, is to predict sales for mnay product families for stores in ecuador.\n",
    "#Train and test share: date, store_nbr, family(product_family), onpromotion (number of products of the family that were on promotion)\n",
    "#Train also has the sales.\n",
    "#Test is what we are trying to predict in the competition\n",
    "\n",
    "#Stores contains store metadata, we might want to join that data into our train and test tables to improve predictions if they are not redundant with the store_nbr (aka if at least some of the stores share their values).\n",
    " #stores metadata\n",
    "#type and cluster are not redundant. see stores_df[['type', 'cluster']].drop_duplicates()#.pivot(columns='cluster')\n",
    "\n",
    "#Daily oil price, keep in mind oil price affects ecuador economical health.\n",
    "#We probably want to join it into our dataframe, but keep in mind it seems to have some null values. We probably want to fill downwards so that it takes its value from the previous date but well have to see. We could also average between the next and previous day.\n",
    "\n",
    "\n",
    "#While its not clear it seems to be the total transactions of the store on that day\n",
    "\n",
    "\n",
    "#A table containing holiday events. \n",
    "#its worth noting that some events were transfered in date so we need to take into account hte actual date.\n",
    "#Also, keep in mind that some seem to be regional and some local. This could mean that we need to take into account the region in considering them.\n",
    "#Some holidays were of type bridge, meaning  they are extra days added to the same holiday.\n",
    "#This are often compensated by making some not working days(ie saturday) working days. This are of type workday.\n",
    "\n",
    "#Additonally to these tables wages on the public sector are paid on the 15th and last day of each month which could affect supermarket sales\n",
    "#A great earthquiake struck ecuador on april 16 2016 which resulted in donations possibly affecting sales.\n",
    "\n",
    "\n",
    "#TLDR we need to add many features to the train and test df.\n",
    "#First lets make some column names easier to understand. Specially after joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "with open('./.kaggle/kaggle.json') as credentials_file:\n",
    "    credentials_dict = json.load(credentials_file)\n",
    "    os.environ['KAGGLE_USERNAME'] = credentials_dict['username']\n",
    "    os.environ['KAGGLE_KEY'] = credentials_dict['key']\n",
    "    import kaggle\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset():\n",
    "    kaggle.api.competition_download_files(competition='store-sales-time-series-forecasting', path='./dataset', force=False, quiet=True)\n",
    "    with ZipFile('./dataset/store-sales-time-series-forecasting.zip') as dataset_zip:\n",
    "        dataset_zip.extractall('./dataset')\n",
    "\n",
    "def get_raw_dfs() ->tuple:\n",
    "    \"\"\"Get dataframes containing all the info in the dataset.\"\"\"\n",
    "    train_df_inc = pd.read_csv('./dataset/train.csv', index_col='id')\n",
    "    test_df_inc = pd.read_csv('./dataset/test.csv', index_col='id')\n",
    "    stores_df = pd.read_csv('./dataset/stores.csv', index_col='store_nbr')\n",
    "    oil_df = pd.read_csv('./dataset/oil.csv') \n",
    "    transactions_df = pd.read_csv('./dataset/transactions.csv')\n",
    "    holiday_events_df = pd.read_csv('./dataset/holidays_events.csv')\n",
    "    sample_submission_df = pd.read_csv('./dataset/sample_submission.csv')\n",
    "    return train_df_inc, test_df_inc, stores_df, oil_df, transactions_df, holiday_events_df, sample_submission_df\n",
    "\n",
    "def rename_raw_dfs_cols(train_df_inc, test_df_inc, stores_df, oil_df, transactions_df, holiday_events_df, sample_submission_df) ->tuple:\n",
    "    \"\"\"Rename the columns of the raw dataframes so they are more easily understandable\"\"\"\n",
    "    train_df_inc = train_df_inc.rename(columns={'family':'product_family', 'onpromotion':'products_of_family_on_promotion'})\n",
    "    test_df_inc = test_df_inc.rename(columns={'family':'product_family'})\n",
    "    oil_df = oil_df.rename(columns={'dcoilwtico':'oil_price'})\n",
    "    stores_df = stores_df.rename(columns={'transactions':'all_products_transactions', 'type':'store_type', 'cluster':'store_cluster', 'city':'store_city', 'state':'store_state'})\n",
    "    holiday_events_df = holiday_events_df.rename(columns={'type':'day_type', 'locale':'holiday_locale_type', 'locale_name':'holiday_locale','description':'holiday_description', 'transferred':'holiday_transferred'})\n",
    "    return train_df_inc, test_df_inc, stores_df, oil_df, transactions_df, holiday_events_df, sample_submission_df\n",
    "\n",
    "def complete_features_dataset(features_df:pd.DataFrame, stores_df:pd.DataFrame, oil_df:pd.DataFrame, transactions_df:pd.DataFrame, holiday_events_df:pd.DataFrame)->pd.DataFrame:\n",
    "    \"\"\"Add relevant columns from the auxiliary dataframes into a features dataset, be it the train set or the test set.\"\"\"\n",
    "    full_features_df = features_df.merge(oil_df, on='date',how='left')\n",
    "    full_features_df = full_features_df.merge(stores_df, on=['store_nbr'], how='left') #Need to also consider date. 'date', nvm\n",
    "    full_features_df = full_features_df.merge(holiday_events_df, on='date')\n",
    "    full_features_df = full_features_df.merge(transactions_df, on=['date', 'store_nbr'])\n",
    "    return full_features_df\n",
    "\n",
    "def reorder_features_dataset(features_df):\n",
    "    \"\"\"Reorder the columns in the feature dataframe so the table becomes easier to understand and inspect. Does not affect the rows.\"\"\"\n",
    "    return features_df # TODO Order to columns more logically\n",
    "\n",
    "def one_hot_encode_features_dataset(features_df):\n",
    "    \"\"\" \"\"\"\n",
    "    return features_df\n",
    "    #TODO #return the dataset and the references\n",
    "\n",
    "def normalize_numerical_features(features_df):\n",
    "    \"\"\" \"\"\"\n",
    "    return features_df\n",
    "    #TODO #return the dataset and the normalizer\n",
    "\n",
    "def fill_missing_values(features_df):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "\n",
    "def prepare_features_df(inc_dataframe:pd.DataFrame, stores_df, oil_df, transactions_df, holiday_events_df):\n",
    "    #TODO READ and rename columns of the dataset, and then process them here.\n",
    "    complete_features_df = complete_features_dataset(inc_dataframe)\n",
    "    ordered_features_df = reorder_features_dataset(complete_features_df)\n",
    "    return ordered_features_df\n",
    "\n",
    "def prepare_datasets():\n",
    "    train_df_inc, test_df_inc, stores_df, oil_df, transactions_df, holiday_events_df, sample_submission_df = rename_raw_dfs_cols(get_raw_dfs())\n",
    "    train_y_truth = train_df_inc.pop('sales')\n",
    "    train_df, test_df = complete_features_dataset(train_df_inc), complete_features_dataset(test_df_inc)\n",
    "    return train_df, test_df, train_y_truth, sample_dataset\n",
    "\n",
    "\n",
    "train_df, test_df, train_y_truth, sample_dataset = prepare_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
